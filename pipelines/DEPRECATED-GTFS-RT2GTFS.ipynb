{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from gtfs_realtime_utils import *\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "ROOT = Path(\"../\")\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to unzip the GTFS timetable file to get all the route info. We'll unzip the zip file and store everything in `extract_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS_timetable = ROOT / \"18SepGB_GTFS_Timetables_Downloaded/itm_yorkshire_gtfs.zip\"\n",
    "\n",
    "# Define the directory where you want to extract files from the timetable\n",
    "extract_dir = ROOT / \"18SepGB_GTFS_Timetables_Downloaded/yorkshire\"\n",
    "\n",
    "unzip_file(GTFS_timetable, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "To be able to match the live buses to a trip/route etc, we need a way to lookup a given bus's `trip_id` and find its timetabled information. We'll do this using dictionaries because they have [O(1) lookup time thanks to hashmaps](https://dev.to/ajipelumi/how-dictionary-lookup-operations-are-o1-49pk).\n",
    "\n",
    "We load just the parts we need into pandas dataframes, then use the `to_dict()` method to create our dictionaries for various ID->ID combinations.\n",
    "\n",
    "We can get more detailed information about UK stops from the [National Public Transport Access Codes data](https://beta-naptan.dft.gov.uk/download). We need the bearing to help match buses to stops on the correct side of the road for their direction of travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_names_bearings = get_stop_names_and_bearings()\n",
    "stop_names_bearings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ID parts of various files that we extracted from the GTFS timetable.\n",
    "agencies, routes, stops, stop_times, trips = load_gtfs_ids(extract_dir)\n",
    "\n",
    "# Make the dataframes to be used to make dicts\n",
    "agency2route = agencies.merge(routes, on='agency_id').set_index('agency_id')\n",
    "AgencyNOC2AgencyIDDict = agency2route['agency_noc'].drop_duplicates().to_dict()\n",
    "\n",
    "## Routes have multiple associated trips. Need to think about this. \n",
    "## Not sure I am using this anywhere so maybe just remove\n",
    "# route2trip = routes.merge(trips, on='route_id').set_index('route_id')\n",
    "# RouteID2TripIDDict = route2trip['trip_id'].to_dict()\n",
    "\n",
    "# Each trip_id has multiple associated stops. Aggregate these into a list per trip_id.\n",
    "trip2stoptimes = trips.merge(stop_times, on='trip_id')[['trip_id', 'stop_id']].groupby('trip_id').agg(list)\n",
    "Trip2StopIDDict = trip2stoptimes['stop_id'].to_dict()\n",
    "\n",
    "# There are some stop_id in stops.txt that are not in stop_times.txt. \n",
    "# We can't use these. Our merge removes these as we complete an \"inner\" join.\n",
    "stoptimes2stops = stop_times.merge(stops, on='stop_id').merge(stop_names_bearings, how='inner', on='stop_id')\n",
    "stoptimes2stops.drop_duplicates(subset='stop_id', keep='first', inplace=True) # We only need unique stop_ids\n",
    "stoptimes2stops.set_index('stop_id', inplace=True)\n",
    "StopID2StopLocDict = stoptimes2stops[['stop_lat', 'stop_lon', 'Bearing']].to_dict(orient='index')\n",
    "# print(len(stops.stop_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this worked properly, we can see if the length of the dictionary is the same as the number of unique IDs from the original GTFS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(agencies.agency_id.unique()) == len(AgencyNOC2AgencyIDDict)\n",
    "# assert len(routes.route_id.unique()) == len(RouteID2TripIDDict)\n",
    "assert len(trips.trip_id.unique()) == len(Trip2StopIDDict)\n",
    "# assert len(stoptimes2stops.stop_id.unique()) == len(StopID2StopLocDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the paths of the GTFS-RT files.\n",
    "\n",
    "For each of the extracted GTFS-RT files we get their full paths and save it in `gtfs_rt_file_paths`. This is to save some compute time downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the GTFS-RT file\n",
    "sep_15 = 'data/gtfs-rt/extracted_15SepGB_BusLocations_GTFSRT'\n",
    "sep_18 = 'data/gtfs-rt/extracted_18SepGB_BusLocations_GTFSRT'\n",
    "sep_19 = 'data/gtfs-rt/extracted_19SepGB_BusLocations_GTFSRT'\n",
    "\n",
    "gtfs_rt_dir = ROOT / sep_15\n",
    "\n",
    "# Create an empty list to store file paths\n",
    "gtfs_rt_file_paths = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(gtfs_rt_dir):\n",
    "    for file in files:\n",
    "        # Get the full path of the file and append it to the list\n",
    "        full_path = os.path.abspath(os.path.join(root, file))\n",
    "        gtfs_rt_file_paths.append(full_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the feed entities \n",
    "We load each feed entity into a list (array in most other languages) to loop through later. We could write this in one big loop, but we've split it up to save compute time and make the code easier to follow/debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = gtfs_realtime_pb2.FeedMessage()\n",
    "entities = []\n",
    "for gtfsrt in gtfs_rt_file_paths:\n",
    "    # Read the GTFS-RT file\n",
    "    # Ensure we're reading a binary file and not the readme.md or anything else accidentally.\n",
    "    if '.bin' in str(gtfsrt):\n",
    "        with open(gtfsrt, 'rb') as file:\n",
    "            feed.ParseFromString(file.read())\n",
    "            for entity in feed.entity:\n",
    "                entities.append(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the real-time locations of Buses\n",
    "\n",
    "Now we want to iterate through each minute of the GTFS-RT feed data.\n",
    "- For each feed message, we iterate through every entity. \n",
    "- A single entity contains information about a single trip (bus). \n",
    "- Per entity, we instantiate a `Class` called `BusDetail` and save the current trip info to that class.\n",
    "- If the trip_id exists, we find all the stops on that route using the trip_id.\n",
    "- Get the locations of the stops on that route.\n",
    "- Calculate a bounding box...\n",
    "- Check the bearing using... \n",
    "- Calculate the distance to each stop in the simplified list of stops using the Haversine formula.\n",
    "- Get the `stop_id` and distance of the nearest stop (smallest distance)\n",
    "- If the nearest stop exists and is closer than 200m, we add this bus to our bag, `BusDetailsBag` for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BusDetailsBag = []\n",
    "count = 0\n",
    "no_trip_id = 0\n",
    "t0 = time.time()\n",
    "# For the moment assuming that `vehicle` is passed. there are other mutually exclusive \n",
    "# options: 'trip_update', 'alert', and 'shape'. See the DOCS https://gtfs.org/documentation/realtime/reference/#message-feedentity\n",
    "for entity in entities:\n",
    "    BD = BusDetail()\n",
    "    # These are the two main parts of entity\n",
    "    vehicle = entity.vehicle\n",
    "    # # These are sub parts\n",
    "    trip = vehicle.trip\n",
    "    pos = vehicle.position\n",
    "    # These are individual values\n",
    "    BD.feed_uid = entity.id\n",
    "    BD.trip_id = trip.trip_id\n",
    "    BD.route_id = trip.route_id\n",
    "    BD.lat = round(pos.latitude, 6)\n",
    "    BD.lon = round(pos.longitude, 6)\n",
    "    BD.bearing = pos.bearing\n",
    "    BD.ts = vehicle.timestamp\n",
    "    BD.v_id = vehicle.vehicle.id\n",
    "    BD.occupancy_status = vehicle.occupancy_status\n",
    "    BD.current_stop_sequence = vehicle.current_stop_sequence\n",
    "    BD.current_status = vehicle.current_status\n",
    "\n",
    "    if BD.trip_id:\n",
    "        stops_on_route = Trip2StopIDDict.get(BD.trip_id)\n",
    "        # If stops isn't none - i.e. if this trip ID is part of the WY timetable\n",
    "        if stops_on_route:\n",
    "            # Get the stop_id, lat, and lon for each stop on the route.\n",
    "            # Scaling the longitudes because they get closer together nearer the poles. Could replace BD.lat with cos(~53) for UK average\n",
    "            actual_stop_locations_on_route = [\n",
    "                (stopid, StopID2StopLocDict[stopid]['stop_lat'], \n",
    "                 StopID2StopLocDict[stopid]['stop_lon'], \n",
    "                 abs(StopID2StopLocDict[stopid]['Bearing'] - BD.bearing), \n",
    "                 abs(StopID2StopLocDict[stopid]['stop_lat'] - BD.lat), \n",
    "                 abs((StopID2StopLocDict[stopid]['stop_lon'] - BD.lon)/np.cos(BD.lat))\n",
    "                ) for stopid in stops_on_route\n",
    "            ]\n",
    "            \n",
    "            box_size = 0.003 # 0.01 is ~1.1km \n",
    "            # @TODO ADD BUS BEARING HERE. Can filter out all stops that are pointing the \"wrong direction\". Absolute difference between stop bearing and bus bearing should be < 90 degrees. This gives a semi-circle's worth of error margin.\n",
    "            \n",
    "            stops_in_bounds = [item for item in actual_stop_locations_on_route if (item[4] < box_size) and (item[5] < box_size) and (item[3] < 90) and (item[3] != 'nan')]\n",
    "            \n",
    "            if len(stops_in_bounds) > 0:\n",
    "                candidate_stops_and_distances = [item + (haversine(BD.lat, BD.lon, item[1], item[2]),) for item in stops_in_bounds]\n",
    "    \n",
    "                n_possible_stops = len(candidate_stops_and_distances)\n",
    "                if n_possible_stops == 1:\n",
    "                    # Found the nearest stop already. Get the distance and stop_id.\n",
    "                    closest_stop_id = candidate_stops_and_distances[0][0]\n",
    "                    closest_stop_distance = candidate_stops_and_distances[0][6]\n",
    "                else:\n",
    "                    # Need to get min distance from n stops.\n",
    "                    index_of_smallest_distance = min(range(len(candidate_stops_and_distances)), key=lambda i: candidate_stops_and_distances[i][6])\n",
    "                    closest_stop_id = candidate_stops_and_distances[index_of_smallest_distance][0]\n",
    "                    closest_stop_distance = candidate_stops_and_distances[index_of_smallest_distance][6]\n",
    "                \n",
    "                # Add the details to the current bus.\n",
    "                BD.NearestStopOnRoute = closest_stop_id\n",
    "                BD.NearestStopDistance = closest_stop_distance * 1e3 #convert to m\n",
    "\n",
    "                # Ensure we found a nearest stop and that the bus is \"sufficiently\" close.\n",
    "                if BD.NearestStopOnRoute != None and BD.NearestStopDistance < 200:\n",
    "                    BusDetailsBag.append(BD)\n",
    "        else:\n",
    "            no_trip_id += 1\n",
    "    # Creating some info to see progress\n",
    "    if count!= 0 and count % 500000 == 0:\n",
    "        t1 = time.time()\n",
    "        print('Time elapsed:', round(t1-t0, 3), 's')\n",
    "        print(f\"{count} of {len(entities)} entities parsed.\")\n",
    "    count +=1\n",
    "print(f\"Number of buses with no trip_id: {no_trip_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete duplicate stops (this also gets rid of long periods where the vehicle is waiting)\n",
    "\n",
    "Sort the frame by timestamp descending, then keep FIRST of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'trip_id': bus.trip_id, \"route_id\": bus.route_id, 'timestamp': bus.ts, 'nearest_stop_id': bus.NearestStopOnRoute, 'distance': bus.NearestStopDistance} for bus in BusDetailsBag]\n",
    "\n",
    "# Add the data to a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Work out the human readable time\n",
    "df['human_time'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "# Timezone is currently BST =  UTC + 1, so need to add 1 hour.\n",
    "df['uk_bst_time_only'] = (df['human_time'] + pd.Timedelta(hours=1)).dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Weird rows where they aren't the right date. Binning them\n",
    "df = df.loc[df.human_time.dt.date == pd.to_datetime('2024-09-15').date()]\n",
    "\n",
    "# only remove duplictaes that have same stop_id, route_id and nearest_stop_id\n",
    "df.drop_duplicates(subset=['trip_id', 'route_id', 'nearest_stop_id'], keep='first', inplace=True)\n",
    "\n",
    "# # For a given trip_id, the arrival_time of (n+1)-th stoptime in sequence must not precede the departure_time of n-th stoptime in sequence in stop_times.txt.\n",
    "# df.sort_values(by=['timestamp', 'trip_id', 'route_id'], ascending=True, inplace=True)\n",
    "\n",
    "FilteredOrderedBusLocations = df.copy()\n",
    "# FilteredOrderedBusLocations[FilteredOrderedBusLocations.trip_id == 'VJ79ee6abdf8b1015398ec41a7f77b76c7f6a0b10f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the original GTFS timetable data. We'll use this to pull all the info about the routes based on the trips we were actually able to match buses for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agencies, routes, trips, stops, stop_times, calendar, calendar_dates, feed_info, shapes = load_full_gtfs(extract_dir, include=['feed_info.txt', 'shapes.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write this timetable as GTFS. This includes:\n",
    "- agency.txt\n",
    "- calendar.txt\n",
    "- calendar_dates.txt\n",
    "- routes.txt\n",
    "- stop_times.txt\n",
    "- stops.txt \n",
    "- trips.txt\n",
    "- shapes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_datadir = ROOT / \"war/yorkshire/150924/\"\n",
    "os.makedirs(os.path.abspath(war_datadir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealRouteIDs = FilteredOrderedBusLocations['route_id'].astype(int)\n",
    "RealRoutes = routes[routes['route_id'].isin(RealRouteIDs)]\n",
    "RealRoutes.to_csv(war_datadir / \"routes.txt\", index=False)\n",
    "RealRoutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealAgencyIDs = routes['agency_id']\n",
    "RealAgencies = agencies[agencies['agency_id'].isin(RealAgencyIDs)]\n",
    "RealAgencies.to_csv(war_datadir / \"agency.txt\", index=False)\n",
    "RealAgencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealTripIDs = FilteredOrderedBusLocations['trip_id']\n",
    "RealTrips = trips[trips['trip_id'].isin(RealTripIDs)]\n",
    "RealTrips.to_csv(war_datadir / \"trips.txt\", index=False)\n",
    "RealTrips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealShapeIDs = RealTrips['shape_id'].unique()\n",
    "RealShapes = shapes[shapes.shape_id.isin(RealShapeIDs)]\n",
    "RealShapes.to_csv(war_datadir / \"shapes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealServiceIDs = RealTrips['service_id'].unique()\n",
    "RealCalendar = calendar[calendar['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendarDates = calendar_dates[calendar_dates['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendar.to_csv(war_datadir / \"calendar.txt\", index=False)\n",
    "RealCalendarDates.to_csv(war_datadir / \"calendar_dates.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop times.txt.\n",
    "\n",
    "minimally required fields are trip_id, arrival_time and/or departure_time, stop_id, and stop_sequence (must be sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealTripIDs_list = RealTripIDs.to_list()\n",
    "FilteredStopTimes = stop_times[stop_times.trip_id.isin(RealTripIDs_list)]\n",
    "FilteredOrderedBusLocations.rename(columns={'nearest_stop_id': 'stop_id'}, inplace=True)\n",
    "RealTrips2RealStopsDict = FilteredStopTimes.groupby('trip_id')['stop_id'].agg(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealStopTimes = FilteredStopTimes.merge(FilteredOrderedBusLocations, on=['trip_id', 'stop_id'], how='inner')\n",
    "RealStopTimes['arrival_time'] = RealStopTimes['uk_bst_time_only']\n",
    "RealStopTimes['departure_time'] = RealStopTimes['uk_bst_time_only']\n",
    "\n",
    "# A trip must visit more than one stop in stop_times.txt to be usable by passengers for boarding and alighting.\n",
    "stop_counts = RealStopTimes.groupby('trip_id')['stop_id'].count()\n",
    "trip_ids_with_one_stop = stop_counts[stop_counts == 1].index.to_list()\n",
    "RealStopTimes = RealStopTimes[~RealStopTimes.trip_id.isin(trip_ids_with_one_stop)] # Exclude trips with only 1 stop. (The \"~\" is negation)\n",
    "\n",
    "# When sorted by stop_times.stop_sequence, two consecutive entries in stop_times.txt \n",
    "# should have increasing distance, based on the field shape_dist_traveled. \n",
    "# If the values are equal, this is considered as an error.\n",
    "RealStopTimes.sort_values(by=['trip_id', 'stop_sequence'], ascending=True, inplace=True)\n",
    "\n",
    "# If pick up type == 1, no pickup is available (Guessing this means you can't get on the bus here?)\n",
    "RealStopTimes = RealStopTimes[RealStopTimes.pickup_type != 1]\n",
    "\n",
    "# Remove duplicate rows based on the specified columns but keep the first occurrence\n",
    "RealStopTimes = RealStopTimes.drop_duplicates(subset=['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'shape_dist_traveled'], keep='first')\n",
    "\n",
    "# Filtering only columns we need to write stop_times.txt\n",
    "RealStopTimes = RealStopTimes[['trip_id','arrival_time','departure_time','stop_id','stop_sequence','stop_headsign','pickup_type','drop_off_type','shape_dist_traveled','timepoint']]\n",
    "# RealStopTimes[RealStopTimes.trip_id == \"VJ79ee6abdf8b1015398ec41a7f77b76c7f6a0b10f\"]\n",
    "\n",
    "RealStopTimes.to_csv(war_datadir / \"stop_times.txt\",index=False)\n",
    "# RealStopTimes[RealStopTimes.trip_id == 'VJ8c815c88e8f60f0f1d3f161565a670bf5de81ed6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stops.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealStopIDs = FilteredOrderedBusLocations['stop_id'].to_list()\n",
    "RealStops = stops[stops.stop_id.isin(RealStopIDs)].copy()\n",
    "\n",
    "# Location type must be able to be parsed as an integer.\n",
    "RealStops['location_type'] = RealStops['location_type'].astype('Int64')\n",
    "RealStops.drop(columns='parent_station', inplace=True)\n",
    "RealStops.to_csv(war_datadir / \"stops.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and writing `feed_info.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_info.to_csv(war_datadir / \"feed_info.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to write some code to automaticall zip these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def zip_directory(folder_path, output_dir_path, output_filename):\n",
    "    # Ensure the output filename does not have a .zip extension\n",
    "    if not output_filename.endswith('.zip'):\n",
    "        output_filename += '.zip'\n",
    "    # Join the output directory and filename\n",
    "    output_zip_path = output_dir_path / output_filename\n",
    "    \n",
    "    # Create the zip archive in the specified directory\n",
    "    shutil.make_archive(output_zip_path.with_suffix(''), 'zip', folder_path)\n",
    "    print(f\"Directory '{folder_path}' successfully zipped as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_directory(war_datadir, ROOT / 'war', 'yorkshire_150924.gtfs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus-tracking-JZQiYmLK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
