{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "os.chdir(\"../\")\n",
    "from pathlib import Path\n",
    "from pipelines.utils import load_full_gtfs, convert_to_unix_timestamp, get_stop_names_and_bearings\n",
    "ROOT = Path(os.getcwd())\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BODS data catalogue\n",
    "bdc = pd.read_csv(ROOT / \"web/bodsdatacatalogue/timetables_data_catalogue.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GTFSRT data\n",
    "dates = [f'202409{i}' for i in range(15, 24)]\n",
    "date_strs = [f\"{date[0:4]}-{date[4:6]}-{date[6:8]}\" for date in dates]\n",
    "gtfsrt_data = [[load_full_gtfs(ROOT / f\"data/real/yorkshire_{date}.gtfs.zip\", ['shapes.txt']), date_str] for date, date_str in zip(dates, date_strs)]\n",
    "\n",
    "# Load the timetable\n",
    "tt_agencies, tt_routes, tt_trips, tt_stops, tt_stop_times, tt_calendar, tt_calendar_dates = load_full_gtfs(ROOT / f\"18SepGB_GTFS_Timetables_Downloaded/itm_yorkshire_gtfs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_data(data, item, subset=None, drop_duplicates=True):\n",
    "    ''''''\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if not result.empty:\n",
    "            result = pd.concat([result, data[i][0][item]])\n",
    "        else:\n",
    "            result = data[i][0][item]\n",
    "    \n",
    "    if drop_duplicates:\n",
    "        result.drop_duplicates(subset, inplace=True, keep='first')\n",
    "    \n",
    "    if subset == 'route_id':\n",
    "        result['date_str'] = data[i][1]\n",
    "\n",
    "    return result\n",
    "\n",
    "all_agency = glue_data(gtfsrt_data, 0, drop_duplicates=True, subset='agency_id')\n",
    "all_routes = glue_data(gtfsrt_data, 1, drop_duplicates=True, subset='route_id')\n",
    "all_trips = glue_data(gtfsrt_data, 2, drop_duplicates=False)\n",
    "all_stop_times = glue_data(gtfsrt_data, 4, drop_duplicates=True, subset=['trip_id', 'stop_id', 'stop_sequence'])\n",
    "all_stops = glue_data(gtfsrt_data, 3, drop_duplicates=True, subset='stop_id')\n",
    "all_shapes = glue_data(gtfsrt_data, 7, drop_duplicates=True, subset=['shape_id', 'shape_pt_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_route_names(string: str):\n",
    "    '''\n",
    "    Simplify the route names:\n",
    "        -Remove reference to bus stations. \n",
    "        -Limit to A-z characters.\n",
    "        -Replace spaces with hyphens.\n",
    "        -Replace double hyphens with single.\n",
    "        -Remove trailing hyphens.\n",
    "    '''\n",
    "    bus_station_pattern = r\"(city bus station|bus station)\"\n",
    "    string = re.sub(bus_station_pattern, '', string, flags=re.IGNORECASE)\n",
    "    string = string.strip()\n",
    "    string = re.sub(r'[^a-zA-Z]+', ' ', string)\n",
    "    return string\n",
    "\n",
    "def kebab_case(string:str):\n",
    "    string = re.sub(r' ', '-', string)\n",
    "    string = re.sub(r\"--\", \"-\", string)\n",
    "    string = re.sub(r\"-$\",\"\", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "def get_detailed_route_info(bdc, route_short_name:str, agency_noc:str):\n",
    "    \"\"\"\"\"\"\n",
    "    # Split the line names by a space\n",
    "    data = bdc.copy()\n",
    "    data['XML:Line Name'] = data['XML:Line Name'].str.split(' ')\n",
    "    # Explode the list\n",
    "    exploded_bdc = data.explode('XML:Line Name')\n",
    "    # Filter to one row\n",
    "    filtered_result = exploded_bdc[(exploded_bdc['XML:Line Name'] == route_short_name) & (exploded_bdc['XML:National Operator Code']==agency_noc)]\n",
    "    start = filtered_result['OTC:Start Point'].values[0]\n",
    "    finish = filtered_result['OTC:Finish Point'].values[0]\n",
    "    via = filtered_result['OTC:Via'].values[0]\n",
    "        \n",
    "    names = [start, finish, via]\n",
    "    tidy_names = []\n",
    "    kebab_names = []\n",
    "    for name in names:\n",
    "        if type(name) == str:\n",
    "            tidy_name = tidy_route_names(name)\n",
    "            kebab_name = kebab_case(tidy_name)\n",
    "        else:\n",
    "            tidy_name = name\n",
    "            kebab_name = name\n",
    "        tidy_names.append(tidy_name)\n",
    "        kebab_names.append(kebab_name)\n",
    "\n",
    "    return tidy_names, kebab_names\n",
    "\n",
    "def get_row_info(row):\n",
    "    route_id = row['route_id']\n",
    "    agency_id = row['agency_id']\n",
    "    route_short_name = row['route_short_name']\n",
    "    agency_noc = row['agency_noc']\n",
    "    agency_name = row['agency_name']\n",
    "    date_str = row['date_str']\n",
    "    return route_id, route_short_name, agency_id, agency_noc, agency_name, date_str\n",
    "\n",
    "def get_route_id(agency_id, route_short_name, routes):\n",
    "    return routes[(routes.agency_id == agency_id) & (routes.route_short_name == route_short_name)].route_id.values[0]\n",
    "\n",
    "def get_trips_on_this_route(route_id:str, trips):\n",
    "    return trips[trips.route_id == route_id][['trip_id', 'trip_headsign', 'shape_id']]\n",
    "\n",
    "def get_unique_values_from_column(data, column_name:str):\n",
    "    return data[column_name].unique()\n",
    "\n",
    "def get_items_for_unique_set(data, match_column, unique_set, slice_columns=None, rename=None):\n",
    "    matched_data = data[data[match_column].isin(unique_set)]\n",
    "    if slice_columns:\n",
    "        matched_data_sliced = matched_data.loc[:, slice_columns]\n",
    "    if rename:\n",
    "        matched_data_sliced.rename(columns=rename, inplace=True)\n",
    "    return matched_data_sliced\n",
    "\n",
    "def fix_shapes(data):\n",
    "    data = data.groupby('shape_id').apply(lambda x: x[['shape_pt_lon', 'shape_pt_lat']].values.round(5).tolist(), include_groups=False).reset_index(name='geometry')\n",
    "    return data\n",
    "\n",
    "def create_dict(data, index_col, value_col):\n",
    "    return data.set_index(index_col)[value_col].to_dict()\n",
    "\n",
    "def format_stops(data, stop_bearings):\n",
    "    stops_this_route = None\n",
    "    stops_this_route = data.merge(stop_bearings, on='stop_id', how='inner')\n",
    "    stops_this_route.rename(columns={'stop_name': 'name', 'stop_lat': 'lat', 'stop_lon': 'lon', 'Bearing': 'bearing'}, inplace=True)\n",
    "    stops_this_route['bearing'] = stops_this_route['bearing'].astype(int)\n",
    "    stops_this_route.set_index('stop_id', inplace=True)\n",
    "    stops = stops_this_route.to_dict(orient='index')\n",
    "    return stops\n",
    "\n",
    "def format_trip_list(stop_times_for_this_route):\n",
    "    # Create an empty list to store results\n",
    "    trip_list = []\n",
    "\n",
    "    # Iterate over each unique trip_id\n",
    "    for trip_id in stop_times_for_this_route['trip_id'].unique():\n",
    "        \n",
    "        # Filter rows for the current trip_id\n",
    "        trip_df = stop_times_for_this_route[stop_times_for_this_route['trip_id'] == trip_id]\n",
    "        # Sort by 'real' time\n",
    "        trip_df = trip_df.sort_values(by='real')\n",
    "        # Create a list of dicts for this trip\n",
    "        current_trip_data = []\n",
    "        for i, row in trip_df.iterrows():\n",
    "            trip_data = [\n",
    "                    row['stop_id'],\n",
    "                    int(row['real']),\n",
    "                    int(row['timetable'])\n",
    "            ]\n",
    "            current_trip_data.append(trip_data)\n",
    "        # Append this trip's list to the main list\n",
    "        trip_list.append(current_trip_data)\n",
    "\n",
    "    return trip_list\n",
    "\n",
    "def create_metadata(route_short_name, kebab_start, kebab_finish, route_start, route_via, route_finish, agency_name, agency_noc):\n",
    "    # print(route_via, type(route_via))\n",
    "    if route_via == np.nan:\n",
    "        name = f\"{route_short_name} - {route_start} - {route_via} - {route_finish}\"\n",
    "    else:\n",
    "        name = f\"{route_short_name} - {route_start} - {route_finish}\"\n",
    "    return dict({'id': f\"{route_short_name}-{kebab_start}-{kebab_finish}\", \n",
    "                 'name': name, \n",
    "                 \"agency_name\": agency_name, \"agency_noc\": agency_noc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = None\n",
    "formatted_stops = None\n",
    "trip_list = None\n",
    "line = None\n",
    "routes2agency = all_routes.merge(all_agency, on='agency_id', how='inner')\n",
    "for i, row in routes2agency.iterrows():\n",
    "    route_id, route_short_name, agency_id, agency_noc, agency_name, date_str = get_row_info(row)\n",
    "    # print(route_id, agency_id, route_short_name, agency_noc)\n",
    "    try:\n",
    "        human_names, kebab_names = get_detailed_route_info(bdc, route_short_name, agency_noc)\n",
    "        route_start, route_finish, route_via = human_names\n",
    "        kebab_start, kebab_finish, kebab_via = kebab_names\n",
    "    except:\n",
    "        print(\"Unable to get route start and end. Skipping...\")\n",
    "        continue\n",
    "   \n",
    "    route_id = get_route_id(agency_id, route_short_name, all_routes)\n",
    "   \n",
    "    trips_on_this_route = get_trips_on_this_route(route_id, all_trips)\n",
    "    unique_trips = get_unique_values_from_column(trips_on_this_route, 'trip_id')\n",
    "    unique_shapes = get_unique_values_from_column(trips_on_this_route, 'shape_id')\n",
    "    # print(\"Unique trips on this route:\\n\\n\", trips_on_this_route.count().to_csv())\n",
    "\n",
    "    # Get the timetabled trips that match the real trips\n",
    "    # print(\"Getting stop times from the timetable...\")\n",
    "    tt_stop_times_for_this_route = get_items_for_unique_set(tt_stop_times, 'trip_id', unique_trips, slice_columns=['trip_id', 'arrival_time', 'stop_id', 'stop_sequence'], rename={'arrival_time': 'timetable'})\n",
    "    \n",
    "    # print(\"Getting shapes for the trips on this route...\")\n",
    "    shapes_on_this_route = get_items_for_unique_set(all_shapes, 'shape_id', unique_shapes, slice_columns=['shape_id', 'shape_pt_lon', 'shape_pt_lat'])\n",
    "\n",
    "    try:\n",
    "        shapes_on_this_route = fix_shapes(shapes_on_this_route)\n",
    "        line = create_dict(shapes_on_this_route, 'shape_id', 'geometry')\n",
    "    except:\n",
    "        print(f'No shape ID available for {route_id, route_short_name, agency_name}')\n",
    "        line = 'na'\n",
    "\n",
    "    # print(\"Getting the stop times for this route...\")\n",
    "    stop_times_for_this_route = get_items_for_unique_set(all_stop_times, 'trip_id', unique_trips, slice_columns=['trip_id', 'arrival_time', 'stop_id', 'stop_sequence'], rename={'arrival_time': 'real'})\n",
    "    unique_stops = get_unique_values_from_column(stop_times_for_this_route, 'stop_id')\n",
    "\n",
    "    # Add the timetabled times\n",
    "    stop_times_for_this_route = stop_times_for_this_route.merge(tt_stop_times_for_this_route, on=['trip_id', 'stop_id', 'stop_sequence'], how='inner')\n",
    "\n",
    "    # Convert times to UTC.\n",
    "    stop_times_for_this_route['real'] = stop_times_for_this_route['real'].apply(convert_to_unix_timestamp, args=(date_str,))\n",
    "    stop_times_for_this_route['timetable'] = stop_times_for_this_route['timetable'].apply(convert_to_unix_timestamp, args=(date_str,))\n",
    "    \n",
    "    trip_list = format_trip_list(stop_times_for_this_route)\n",
    "\n",
    "    # Stops\n",
    "    stops_this_route = get_items_for_unique_set(all_stops, 'stop_id', unique_stops, slice_columns=['stop_id', 'stop_name', 'stop_lon', 'stop_lat'])\n",
    "    stop_bearings = get_stop_names_and_bearings()[['stop_id', 'Bearing']]\n",
    "\n",
    "    formatted_stops = format_stops(stops_this_route, stop_bearings)\n",
    "\n",
    "    meta = create_metadata(route_short_name, kebab_start, kebab_finish, route_start, route_via, route_finish, agency_name, agency_noc)\n",
    "    \n",
    "    content = dict({'meta': meta, 'line': line, 'stops': formatted_stops, 'trips': trip_list})\n",
    "    with open(ROOT / f\"web/TLE/{route_short_name}-{kebab_start}-{kebab_finish}.json\", \"w\") as f:\n",
    "        json.dump(content, f, separators=(',',':'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus-tracking-JZQiYmLK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
