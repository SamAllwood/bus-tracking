{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "os.chdir(\"../\")\n",
    "from pathlib import Path\n",
    "from pipelines.utils import load_full_gtfs, convert_to_unix_timestamp, get_stop_names_and_bearings\n",
    "ROOT = Path(os.getcwd())\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BODS data catalogue\n",
    "bdc = pd.read_csv(ROOT / \"web/bodsdatacatalogue/timetables_data_catalogue.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'north_west'\n",
    "rgncd = 'TLD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GTFSRT data\n",
    "dates = [f'202409{i}' for i in range(15, 24)]\n",
    "date_strs = [f\"{date[0:4]}-{date[4:6]}-{date[6:8]}\" for date in dates]\n",
    "gtfsrt_data = [[load_full_gtfs(ROOT / f\"data/real/{region}_{date}.gtfs.zip\", ['shapes.txt']), date_str] for date, date_str in zip(dates, date_strs)]\n",
    "\n",
    "# Load the timetable\n",
    "tt_agencies, tt_routes, tt_trips, tt_stops, tt_stop_times, tt_calendar, tt_calendar_dates = load_full_gtfs(ROOT / f\"18SepGB_GTFS_Timetables_Downloaded/itm_{region}_gtfs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_data(data, item, subset=None, drop_duplicates=True):\n",
    "    ''''''\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if not result.empty:\n",
    "            result = pd.concat([result, data[i][0][item]])\n",
    "\n",
    "            if item==4:\n",
    "                result['date_str'] = result['date_str'].fillna(data[i][1])\n",
    "                \n",
    "        else:\n",
    "            result = data[i][0][item]\n",
    "            if item==4:\n",
    "                result['date_str'] = data[i][1]\n",
    "            \n",
    "    if drop_duplicates:\n",
    "        result.drop_duplicates(subset, inplace=True, keep='first')\n",
    "\n",
    "    return result\n",
    "\n",
    "all_agency = glue_data(gtfsrt_data, 0, drop_duplicates=True, subset='agency_id')\n",
    "all_routes = glue_data(gtfsrt_data, 1, drop_duplicates=True, subset='route_id')\n",
    "all_trips = glue_data(gtfsrt_data, 2, drop_duplicates=False)\n",
    "all_stop_times = glue_data(gtfsrt_data, 4, drop_duplicates=True, subset=['trip_id', 'stop_id', 'stop_sequence', 'date_str'])\n",
    "all_stops = glue_data(gtfsrt_data, 3, drop_duplicates=True, subset='stop_id')\n",
    "all_calendars = glue_data(gtfsrt_data, 5, drop_duplicates=True, subset='service_id')\n",
    "all_shapes = glue_data(gtfsrt_data, 7, drop_duplicates=True, subset=['shape_id', 'shape_pt_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_route_names(string: str):\n",
    "    '''\n",
    "    Simplify the route names:\n",
    "        -Remove reference to bus stations. \n",
    "        -Limit to A-z characters.\n",
    "        -Replace spaces with hyphens.\n",
    "        -Replace double hyphens with single.\n",
    "        -Remove trailing hyphens.\n",
    "    '''\n",
    "    bus_station_pattern = r\"(city bus station|bus station)\"\n",
    "    string = re.sub(bus_station_pattern, '', string, flags=re.IGNORECASE)\n",
    "    string = string.strip()\n",
    "    string = re.sub(r'[^a-zA-Z]+', ' ', string)\n",
    "    return string\n",
    "\n",
    "def kebab_case(string:str):\n",
    "    string = re.sub(r' ', '-', string)\n",
    "    string = re.sub(r\"--\", \"-\", string)\n",
    "    string = re.sub(r\"-$\",\"\", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "def get_detailed_route_info(bdc, route_short_name:str, agency_noc:str):\n",
    "    \"\"\"\"\"\"\n",
    "    # Split the line names by a space\n",
    "    data = bdc.copy()\n",
    "    data['XML:Line Name'] = data['XML:Line Name'].str.split(' ')\n",
    "    # Explode the list\n",
    "    exploded_bdc = data.explode('XML:Line Name')\n",
    "    # Filter to one row\n",
    "    filtered_result = exploded_bdc[(exploded_bdc['XML:Line Name'] == route_short_name) & (exploded_bdc['XML:National Operator Code']==agency_noc)]\n",
    "    start = filtered_result['OTC:Start Point'].values[0]\n",
    "    finish = filtered_result['OTC:Finish Point'].values[0]\n",
    "    via = filtered_result['OTC:Via'].values[0]\n",
    "        \n",
    "    names = [start, finish, via]\n",
    "    tidy_names = []\n",
    "    kebab_names = []\n",
    "    for name in names:\n",
    "        if type(name) == str:\n",
    "            tidy_name = tidy_route_names(name)\n",
    "            kebab_name = kebab_case(tidy_name)\n",
    "        else:\n",
    "            tidy_name = ''\n",
    "            kebab_name = ''\n",
    "        tidy_names.append(tidy_name)\n",
    "        kebab_names.append(kebab_name)\n",
    "\n",
    "    return tidy_names, kebab_names\n",
    "\n",
    "def get_row_info(row):\n",
    "    route_id = row['route_id']\n",
    "    agency_id = row['agency_id']\n",
    "    route_short_name = row['route_short_name']\n",
    "    agency_noc = row['agency_noc']\n",
    "    agency_name = row['agency_name']\n",
    "    return route_id, route_short_name, agency_id, agency_noc, agency_name\n",
    "\n",
    "def get_route_id(agency_id, route_short_name, routes):\n",
    "    return routes[(routes.agency_id == agency_id) & (routes.route_short_name == route_short_name)].route_id.values[0]\n",
    "\n",
    "def get_trips_on_this_route(route_id:str, trips):\n",
    "    return trips[trips.route_id == route_id][['trip_id', 'trip_headsign', 'shape_id']]\n",
    "\n",
    "def get_unique_values_from_column(data, column_name:str):\n",
    "    return data[column_name].unique()\n",
    "\n",
    "# def get_items_for_unique_set(data, match_column, unique_set, slice_columns=None, rename=None):\n",
    "#     matched_data = data[data[match_column].isin(unique_set)]\n",
    "#     if slice_columns:\n",
    "#         matched_data_sliced = matched_data.loc[:, slice_columns]\n",
    "#     if rename:\n",
    "#         matched_data_sliced.rename(columns=rename, inplace=True)\n",
    "#     return matched_data_sliced\n",
    "\n",
    "# def fix_shapes(data):\n",
    "#     data = data.groupby('shape_id').apply(lambda x: x[['shape_pt_lon', 'shape_pt_lat']].values.round(5).tolist(), include_groups=False).reset_index(name='geometry')\n",
    "#     return data\n",
    "\n",
    "# def create_dict(data, index_col, value_col):\n",
    "#     return data.set_index(index_col)[value_col].to_dict()\n",
    "\n",
    "# def format_stops(data, stop_bearings):\n",
    "#     stops_this_route = None\n",
    "#     stops_this_route = data.merge(stop_bearings, on='stop_id', how='inner')\n",
    "#     stops_this_route.rename(columns={'stop_name': 'name', 'stop_lat': 'lat', 'stop_lon': 'lon', 'Bearing': 'bearing'}, inplace=True)\n",
    "#     stops_this_route['bearing'] = stops_this_route['bearing'].astype(int)\n",
    "#     stops_this_route.set_index('stop_id', inplace=True)\n",
    "#     stops = stops_this_route.to_dict(orient='index')\n",
    "#     return stops\n",
    "\n",
    "# def format_trip_list(stop_times_for_this_route):\n",
    "    # Create an empty list to store results\n",
    "    trip_list = []\n",
    "\n",
    "    # Iterate over each unique trip_id\n",
    "    for trip_id in stop_times_for_this_route['trip_id'].unique():\n",
    "        \n",
    "        # Filter rows for the current trip_id\n",
    "        trip_df = stop_times_for_this_route[stop_times_for_this_route['trip_id'] == trip_id]\n",
    "        # Sort by 'real' time\n",
    "        trip_df = trip_df.sort_values(by='real')\n",
    "        # Create a list of dicts for this trip\n",
    "        current_trip_data = []\n",
    "        for i, row in trip_df.iterrows():\n",
    "            trip_data = [\n",
    "                    row['stop_id'],\n",
    "                    int(row['real']),\n",
    "                    int(row['timetable'])\n",
    "            ]\n",
    "            current_trip_data.append(trip_data)\n",
    "        # Append this trip's list to the main list\n",
    "        trip_list.append(current_trip_data)\n",
    "\n",
    "    return trip_list\n",
    "\n",
    "def create_metadata(route_short_name, kebab_start, kebab_finish, route_start, route_via, route_finish, agency_name, agency_noc):\n",
    "    # print(route_via, type(route_via))\n",
    "    if route_via and route_start and route_finish:\n",
    "        name = f\"{route_short_name} - {route_start} - {route_finish}\"\n",
    "\n",
    "    elif not route_start and not route_finish:\n",
    "        name = f\"{route_short_name}\"\n",
    "\n",
    "    else:\n",
    "        name = f\"{route_short_name} - {route_start} - {route_via} - {route_finish}\"\n",
    "    \n",
    "    if not kebab_start and not kebab_finish:\n",
    "        id = f\"{route_short_name}\"\n",
    "\n",
    "    else: \n",
    "        id = f\"{route_short_name}-{kebab_start}-{kebab_finish}\"\n",
    "        \n",
    "    return dict({'id': id, \n",
    "                 'name': name, \n",
    "                 \"agency_name\": agency_name, \"agency_noc\": agency_noc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes2agency = all_routes.merge(all_agency, on='agency_id', how='inner')\n",
    "all_routes_dict = routes2agency.set_index('route_id').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_times = all_stop_times[['trip_id', 'stop_id', 'stop_sequence', 'arrival_time', 'date_str']]\n",
    "tt_stop_times = tt_stop_times[['trip_id', 'stop_id', 'stop_sequence', 'arrival_time']]\n",
    "all_stop_times = all_stop_times.merge(tt_stop_times, on=['trip_id', 'stop_id', 'stop_sequence'], how='inner', suffixes=('_real', '_timetable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip2stoptimes = all_stop_times.groupby(['trip_id', 'date_str'])[['arrival_time_real', 'arrival_time_timetable', 'stop_id', 'stop_sequence', 'date_str']].agg(list).to_dict(orient='index')\n",
    "shape_dict = all_shapes.groupby('shape_id').apply(lambda x: x[['shape_pt_lon', 'shape_pt_lat']].values.round(5).tolist(), include_groups=False).reset_index(name='geometry').set_index('shape_id').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bearings = get_stop_names_and_bearings()[['stop_id', 'Bearing']] \n",
    "stops_dict = all_stops.merge(stop_bearings, on='stop_id', how='inner').set_index('stop_id').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in all_routes_dict.items():\n",
    "    route_id = key\n",
    "    agency_id, route_short_name, agency_noc, agency_name = values['agency_id'], values['route_short_name'], values['agency_noc'], values['agency_name']\n",
    "\n",
    "    try:\n",
    "        human_names, kebab_names = get_detailed_route_info(bdc, route_short_name, agency_noc)\n",
    "        route_start, route_finish, route_via = human_names\n",
    "        kebab_start, kebab_finish, kebab_via = kebab_names\n",
    "    except:\n",
    "        # print(\"Unable to get route start and end. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    meta = create_metadata(route_short_name, kebab_start, kebab_finish, route_start, route_via, route_finish, agency_name, agency_noc)\n",
    "    \n",
    "    trips_on_this_route = get_trips_on_this_route(route_id, all_trips)\n",
    "    unique_trips = get_unique_values_from_column(trips_on_this_route, 'trip_id')\n",
    "    trips = []\n",
    "    # Adding every stop id on this route to a list, so that we can find that set from it later on in the code.\n",
    "    all_stop_ids = []\n",
    "\n",
    "    for trip_id, ds in itertools.product(unique_trips, date_strs):\n",
    "        try:\n",
    "            stop_info = trip2stoptimes[trip_id, ds]\n",
    "        except KeyError:\n",
    "            # print(f'No real time info for trip_id:{trip_id}, route_id:{route_id}, agency_name:{agency_name}, route number: {route_short_name}')\n",
    "            continue\n",
    "        tt_arrival_times = stop_info['arrival_time_timetable']\n",
    "        real_arrival_times = stop_info['arrival_time_real']\n",
    "        real_stop_ids = stop_info['stop_id']\n",
    "        stop_dates = stop_info['date_str']\n",
    "       \n",
    "        real_timestamps = [convert_to_unix_timestamp(p, q) for p, q in zip (real_arrival_times, stop_dates)]\n",
    "        tt_timestamps = [convert_to_unix_timestamp(p, q) for p, q in zip (tt_arrival_times, stop_dates)]\n",
    "        # print(len(real_stop_ids), len(real_timestamps), len(tt_timestamps))\n",
    "        trips.append([[i, j, k] for i, j, k in zip(real_stop_ids, real_timestamps, tt_timestamps)])\n",
    "        all_stop_ids.append(real_stop_ids)\n",
    "    \n",
    "    unique_shapes = get_unique_values_from_column(trips_on_this_route, 'shape_id')\n",
    "    line = dict()\n",
    "    for shape_id in unique_shapes:\n",
    "        try:\n",
    "            s = shape_dict[shape_id]\n",
    "            if shape_id not in line:\n",
    "                line[shape_id] = s['geometry']\n",
    "        except:\n",
    "            # print(f'Shape ID was {shape_id}, type {type(shape_id)}')\n",
    "            continue\n",
    "    \n",
    "    flat_stop_list = [v for j in all_stop_ids for v in j]\n",
    "    unique_stops = set(flat_stop_list)\n",
    "    stops = dict()\n",
    "    for s in unique_stops:\n",
    "        stops[s] = dict({\"name\": stops_dict[s]['stop_name'], \"lon\": stops_dict[s]['stop_lon'], \"lat\": stops_dict[s]['stop_lat'], \"bearing\": int(stops_dict[s]['Bearing'])})\n",
    "    \n",
    "    content = dict({'meta': meta, 'line': line, 'stops': stops, 'trips': trips})\n",
    "    if not kebab_start and not kebab_finish:\n",
    "        fname = f\"web/{rgncd}/{route_short_name}.json\"\n",
    "    else:\n",
    "        fname = f\"web/{rgncd}/{route_short_name}-{kebab_start}-{kebab_finish}.json\"\n",
    "    with open(ROOT / fname, \"w\") as f:\n",
    "        json.dump(content, f, separators=(',',':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tt_trips.merge(tt_calendar, on='service_id', how='inner')\n",
    "p = p[(p.start_date >= 20240915) & (p.start_date <= 20240923)]\n",
    "p\n",
    "# print(len(p.trip_id.unique()))\n",
    "# d = all_trips.merge(all_calendars, on='service_id', how='inner')\n",
    "# d = d[(d.start_date >= 20240915) & (d.start_date <= 20240923)]\n",
    "# print(len(d.trip_id.unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus-tracking-JZQiYmLK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
